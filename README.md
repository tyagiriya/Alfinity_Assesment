# Alfinity_Assesment


Suppose we have a text file after this i have passed text file to tokenize function.
It split the text file into token .
I have provided a set of words that indicates racial slurs.
At the end i have calculated rate: number of occurrences normalized by total number.
